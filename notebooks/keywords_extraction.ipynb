{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../data/wine_cleaned_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a list of stopwords for getting descriptors\n",
    "stopwords = nlp.Defaults.stop_words\n",
    "\n",
    "# add more stopwords that I found through examples\n",
    "stopwords |= {\n",
    "    \"aroma\",\n",
    "    \"aromas\",\n",
    "    \"flavor\",\n",
    "    \"flavors\",\n",
    "    \"note\",\n",
    "    \"notes\",\n",
    "    \"food\",\n",
    "    \"touch\",\n",
    "    \"wine\",\n",
    "    \"it's\",\n",
    "}\n",
    "\n",
    "\n",
    "def get_descriptors(string):\n",
    "    \"\"\"\n",
    "    This function uses tools provided by spaCy to grab all adjectives and noun chunks in the string that describes a wine.\n",
    "    Returns all descriptors as a list.\n",
    "    \"\"\"\n",
    "\n",
    "    string = string.lower()  # make all descriptors lower-case\n",
    "    spacy_tokens = nlp(\n",
    "        string\n",
    "    )  # use spaCy to tokenize the string, comes with `token.{pos_, lemma_}` that I will use\n",
    "    noun_chunks = spacy_tokens.noun_chunks  # get all noun chunks in the string\n",
    "\n",
    "    toReturn = []  # initialize list of descriptors to return\n",
    "    for chunk in noun_chunks:\n",
    "        if (\n",
    "            all(\n",
    "                (str(token) not in stopwords)\n",
    "                and token.is_punct  # no token in the noun chunk can be a stopword\n",
    "                != True\n",
    "                and \"-PRON-\"  # no token in the noun chunk can be punctuation\n",
    "                not in token.lemma_  # no token in the noun chunk can be a pronoun\n",
    "                for token in chunk  # conditions above must hold for each token in the noun chunk\n",
    "            )\n",
    "            == True\n",
    "        ):\n",
    "            toReturn.append(str(chunk))  # then this noun chunk can be returned\n",
    "\n",
    "    # there are still stand-alone adjectives which weren't used to describe any nouns but not returned, I want them too\n",
    "    # below is the solution\n",
    "\n",
    "    already_in_noun_chunks = [\n",
    "        word for token in toReturn for word in str(token).split()\n",
    "    ]  # get all words that the noun chunks already contain into a list\n",
    "    for token in spacy_tokens:\n",
    "        if (token.pos_ == \"ADJ\") & (\n",
    "            str(token) not in already_in_noun_chunks\n",
    "        ):  # if a token in the string is an adjective and not already in the list to be returned\n",
    "            toReturn.append(str(token))\n",
    "\n",
    "    return list(toReturn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105554/105554 [13:19<00:00, 132.09it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "df[\"keywords\"] = df[\"description\"].progress_apply(get_descriptors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"keywords\", \"region_variety\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupby(\"region_variety\").agg(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "584it [00:00, 2528.14it/s]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "keywords = {}\n",
    "for i, row in tqdm(df.iterrows()):\n",
    "    counter = Counter(row[\"keywords\"])\n",
    "    top10 = counter.most_common(10)\n",
    "    keywords[row[\"region_variety\"]] = {\n",
    "        \"keywords\": [word for word, _ in top10],\n",
    "        \"counts\": [count for _, count in top10],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../models/idx_to_label.json\", \"r\") as f:\n",
    "    idx_to_label = json.load(f)\n",
    "\n",
    "idx_to_keywords = [\n",
    "    {\n",
    "        \"label\": label,\n",
    "        \"keywords\": keywords[label][\"keywords\"],\n",
    "        \"counts\": keywords[label][\"counts\"],\n",
    "    }\n",
    "    for label in idx_to_label\n",
    "]\n",
    "\n",
    "with open(\"../models/idx_to_keywords.json\", \"w\") as f:\n",
    "    json.dump(idx_to_keywords, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b09ec625f77bf4fd762565a912b97636504ad6ec901eb2d0f4cf5a7de23e1ee5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
