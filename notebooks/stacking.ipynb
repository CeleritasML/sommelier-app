{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load keywords and labels\n",
    "\n",
    "data = {\n",
    "    \"train\": pd.concat([\n",
    "        pd.read_csv(\"../data/wine_keywords_train.csv\"),\n",
    "        pd.read_csv(\"../data/wine_keywords_val.csv\")\n",
    "    ]).dropna(),\n",
    "    \"test\": pd.read_csv(\"../data/wine_keywords_test.csv\").dropna()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keywords</th>\n",
       "      <th>region_variety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>core adequate acidity moderate extraction medi...</td>\n",
       "      <td>France-Languedoc-Roussillon:Cabernet Sauvignon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>complexity varietal character black plum light...</td>\n",
       "      <td>US-California:Merlot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rhubarb cranberry fruit red apple light simple...</td>\n",
       "      <td>US-Oregon:Pinot Noir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>impressive fullness ripeness black cherry leat...</td>\n",
       "      <td>Italy-Veneto:Corvina, Rondinella, Molinara</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dusty tones mineral saffron pollen concentrate...</td>\n",
       "      <td>Germany-Mosel:Riesling</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            keywords  \\\n",
       "0  core adequate acidity moderate extraction medi...   \n",
       "1  complexity varietal character black plum light...   \n",
       "2  rhubarb cranberry fruit red apple light simple...   \n",
       "3  impressive fullness ripeness black cherry leat...   \n",
       "4  dusty tones mineral saffron pollen concentrate...   \n",
       "\n",
       "                                   region_variety  \n",
       "0  France-Languedoc-Roussillon:Cabernet Sauvignon  \n",
       "1                            US-California:Merlot  \n",
       "2                            US-Oregon:Pinot Noir  \n",
       "3      Italy-Veneto:Corvina, Rondinella, Molinara  \n",
       "4                          Germany-Mosel:Riesling  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"train\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(data[\"train\"][\"keywords\"])\n",
    "\n",
    "train_vectors = vectorizer.transform(data[\"train\"][\"keywords\"])\n",
    "test_vectors = vectorizer.transform(data[\"test\"][\"keywords\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer #features: 18261\n",
      "Vectorizer features: ['agoston', 'agreeability', 'agreeable', 'agressive', 'agricoltori', 'agriculture', 'agrinatura', 'agro', 'agua', 'aguia', 'agustin', 'ahi', 'aid', 'aidil', 'aids', 'aiken', 'aims', 'aiolo', 'air', 'airborne', 'aires', 'airfield', 'airiness', 'airing', 'airs', 'airtime', 'airy', 'airén', 'aix', 'aka', 'akin', 'al', 'alabaster', 'alain', 'alamos', 'alan', 'alana', 'alance', 'alarid', 'alarming', 'alaska', 'alastro', 'alayt', 'alazan', 'alba', 'alban', 'albana', 'albanello', 'albar', 'albarino', 'albariño', 'albarossa', 'albe', 'albeggio', 'albera', 'alberdi', 'albert', 'alberta', 'alberto', 'albola', 'alcamo', 'alcantara', 'alchemist', 'alchemy', 'alcholic', 'alcineo', 'alcohol', 'alcoholic', 'alconte', 'aldegheri', 'alder', 'alderbrook', 'ale', 'aleatico', 'alejandro', 'alella', 'alene', 'alentejano', 'alentejo', 'aleramico', 'alert', 'alessandro', 'alessano', 'alessio', 'alex', 'alexander', 'alexandra', 'alexandre', 'alexandria', 'alexandrine', 'alexia', 'alexis', 'alfalfa', 'alfonso', 'alfred', 'alfredo', 'alfresco', 'alfrocheiro', 'aliança', 'alicante']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rqiu/Library/Caches/pypoetry/virtualenvs/sommelier-app-LnLhopHO-py3.9/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "print(\"Vectorizer #features:\", len(vectorizer.get_feature_names()))\n",
    "print(\"Vectorizer features:\", vectorizer.get_feature_names()[500:600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from sklearn.metrics import accuracy_score, make_scorer, top_k_accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Create a custom function to evaluate the list of models:\n",
    "\n",
    "top_k_accuracy_score = make_scorer(top_k_accuracy_score, k=5, needs_threshold=True)\n",
    "\n",
    "def eval_model(model, X, y, n_jobs=-1):\n",
    "    \"\"\"Evaluate a list of models using cross-validation.\n",
    "    \n",
    "    Args:\n",
    "        models (dict): A dictionary of models to evaluate.\n",
    "        X (array-like): Training data.\n",
    "        y (array-like): Training labels.\n",
    "        \n",
    "    Returns:\n",
    "        scores (list): Dictionary of scores (another dict) for each model.\n",
    "    \"\"\"\n",
    "    # for name, model in tqdm(models.items()):\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "    cv_scores = cross_validate(model, X, y, cv=cv, scoring=top_k_accuracy_score, n_jobs=n_jobs, verbose=1, return_train_score=True)\n",
    "\n",
    "    print(f\"[Train] Top-5 prediction mean accuracy: {cv_scores['train_score'].mean():.3f} (+/- {cv_scores['train_score'].std() * 2:.3f})\")\n",
    "    print(f\"[Test] Top-5 prediction mean accuracy: {cv_scores['test_score'].mean():.3f} (+/- {cv_scores['test_score'].std() * 2:.3f})\")\n",
    "    return cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "# Create a dictionary to collect model metrics\n",
    "scores = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 10 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train mean accuracy: 0.354 (+/- 0.002)\n",
      "Test mean accuracy: 0.316 (+/- 0.003)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.9s finished\n"
     ]
    }
   ],
   "source": [
    "# evaluate naïve bayes\n",
    "nb = MultinomialNB()\n",
    "scores[\"nb\"] = eval_model(nb, train_vectors, data[\"train\"][\"region_variety\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 10 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train mean accuracy: 0.576 (+/- 0.011)\n",
      "Test mean accuracy: 0.308 (+/- 0.013)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   44.8s finished\n"
     ]
    }
   ],
   "source": [
    "# evaluate perceptron\n",
    "perceptron = Perceptron(random_state=random_state, early_stopping=True)\n",
    "scores[\"perceptron\"] = eval_model(perceptron, train_vectors, data[\"train\"][\"region_variety\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 10 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train mean accuracy: 0.856 (+/- 0.001)\n",
      "Test mean accuracy: 0.414 (+/- 0.005)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  1.9min finished\n"
     ]
    }
   ],
   "source": [
    "# evaluate linear SVM\n",
    "svm = LinearSVC(random_state=random_state)\n",
    "scores[\"svm\"] = eval_model(svm, train_vectors, data[\"train\"][\"region_variety\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 10 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train mean accuracy: 0.240 (+/- 0.004)\n",
      "Test mean accuracy: 0.170 (+/- 0.002)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    7.9s finished\n"
     ]
    }
   ],
   "source": [
    "# evaluate decision tree\n",
    "tree = DecisionTreeClassifier(random_state=random_state, max_depth=20)\n",
    "scores[\"tree\"] = eval_model(tree, train_vectors, data[\"train\"][\"region_variety\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # evaluate adaboost\n",
    "# ada = AdaBoostClassifier(random_state=random_state, n_estimators=100)\n",
    "# scores[\"ada\"] = eval_model(ada, train_vectors, data[\"train\"][\"region_variety\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # evaluate gradient boosting\n",
    "# gb = GradientBoostingClassifier(random_state=random_state, n_estimators=10, max_depth=5)\n",
    "# scores[\"gb\"] = eval_model(gb, train_vectors, data[\"train\"][\"region_variety\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # evaluate XGBoost\n",
    "# xgb = XGBClassifier(random_state=random_state, n_estimators=100, max_depth=20, n_jobs=-1)\n",
    "# scores[\"xgb\"] = eval_model(xgb, train_vectors, data[\"train\"][\"region_variety\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models/scores.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [134], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels/scores.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      4\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(scores, f)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models/scores.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"models/scores.json\", \"w\") as f:\n",
    "    json.dump(scores, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some thoughts on stacking strategies:\n",
    "\n",
    "1. To align with our BERT model, we intentionally selected the top 5 prediction accuracy as our evaluation metrics in training. This is a little tricky. The `make_scorer()` function takes two parameters `needs_proba` and `needs_threshold` which are both `False` by default. But for specific models, we need to turn the parameters on to calculate the similar \n",
    "2. We discarded the ensemble models in stacking, since "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "# level0 = list()\n",
    "# level0.append((\"nb\", nb))\n",
    "# level0.append((\"perceptron\", perceptron))\n",
    "# level0.append((\"svm\", svm))\n",
    "# # level0.append((\"tree\", tree)) # the accuracy is not ideal\n",
    "# # level0.append((\"ada\", ada)) # the accuracy is too low, plus the parameter turning is quite time-consuming in this stage\n",
    "# level0.append((\"gb\", gb))\n",
    "# level0.append((\"xgb\", xgb))\n",
    "\n",
    "# level1 = XGBClassifier(random_state=random_state)\n",
    "# model = StackingClassifier(estimators=level0, final_estimator=level1, cv=5, n_jobs=-1, passthrough=False)\n",
    "# model.fit(train_vectors, data[\"train\"][\"region_variety\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('sommelier-app-LnLhopHO-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "30804d2f67ffdaea00cb5c53926dce42897aabfd7d02ebe37057f3b56b5c9f64"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
